{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Standard Section 7: Bagging and Random Forest\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner<br/>\n",
    "**Section Leaders**: Marios Mattheakis, Abhimanyu (Abhi) Vasishth, Robbert (Rob) Struyven<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will work with a spam email dataset. Our ultimate goal is to be able to build models so that we can predict whether an email is spam or not spam based on word characteristics within each email. We will cover Decision Trees, Bagging, and Random Forest methods and allow you to apply it to the homework.\n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Load in the spam dataset and split the data into train and test.\n",
    "    2. Find the optimal depth for the Decision Tree model and evaluate performance.\n",
    "    3. Fit the Bagging model using multiple bootstrapped datasets and do majority voting. \n",
    "    4. Fit the Random Forest Model and compare with Bagging.\n",
    "    \n",
    "Hopefully after this section you will be able to answer the following questions: \n",
    " - What are decision tree models?\n",
    " - How do we construct them?\n",
    " - How do we visualize them?\n",
    " - What is bagging?\n",
    " - Why does bagging help with overfitting?\n",
    " - Why does bagging help to built more expressive trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "#### The Idea: Decision Trees are just flowcharts and interpretable!\n",
    "\n",
    "<img src=\"data/flowchart.png\" alt=\"how to fix anything\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "It turns out that simple flow charts can be formulated as mathematical models for classification and these models have the properties we desire;\n",
    " - interpretable by humans \n",
    " - have sufficiently complex decision boundaries \n",
    " - the decision boundaries are locally linear, each component of the decision boundary is simple to describe mathematically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "#### Let's review some theory: \n",
    "\n",
    "#### How to build Decision Trees (the Learning Algorithm in words): \n",
    "To learn a decision tree model, we take a greedy approach: \n",
    " 1. Start with an empty decision tree (undivided feature space) \n",
    " 2. Choose the ‚Äòoptimal‚Äô predictor on which to split and choose the ‚Äòoptimal‚Äô threshold value for splitting by applying a **splitting criterion (1)**\n",
    " 3. Recurse on on each new node until **stopping condition (2)** is met\n",
    " \n",
    "For classification, we label each region in the model with the label of the class to which the majority of the points within the region belong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we need a (1) splitting criterion and a (2) stopping condition:\n",
    "\n",
    "  #### (1) Splitting criterion \n",
    "<img src=\"data/split1.png\" alt=\"split1\" width=\"70%\"/>\n",
    "\n",
    "---\n",
    "<img src=\"data/split2.png\" alt=\"split2\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Stopping condition\n",
    "\n",
    "If we don‚Äôt terminate the decision tree learning algorithm manually, the tree will continue to grow until each region defined by the model possibly contains exactly one training point (and the model attains 100% training accuracy). **Not stopping while building a deeper and deeper tree = 100% training accuracy; What will your test accuracy be? What can we do to fix this?**\n",
    "\n",
    "To prevent the **overfitting** from happening, we could \n",
    "- Stop the algorithm at a particular depth. (=**not too deep**)\n",
    "- Don't split a region if all instances in the region belong to the same class. (=**stop when subtree is pure**)\n",
    "- Don't split a region if the number of instances in the sub-region will fall below pre-defined threshold (min_samples_leaf). (=**not too specific/small subtree**)\n",
    "- Don't use to many splits in the tree (=**not too many splits / not too complex global tree**)\n",
    "- Be content with <100% accuracy training set...\n",
    "\n",
    "-------------\n",
    "\n",
    "#### Done with theory, let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "# Part 1 : Introduction to the Spam Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a spam email dataset. The dataset has 57 predictors with a response variable called `Spam` that indicates whether an email is spam or not spam. The goal is to be able to create a classifier or method that acts as a spam filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataframe and Set Column Names\n",
    "spam_df = pd.read_csv('data/spam.csv', header=None)\n",
    "columns = [\"Column_\"+str(i+1) for i in range(spam_df.shape[1]-1)] + ['Spam']\n",
    "spam_df.columns = columns\n",
    "display(spam_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor variabes are all continuous. They represent certain features like the frequency of the word \"`discount`\". The exact specification and description of each predictor can be found online. We are not so much interested in the exact inference of each predictor so we will omit the exact names of each of the predictors. We are more interested in the prediction of the algorithm so we will treat each as predictor without going into too much exact detail in each.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to description : https://archive.ics.uci.edu/ml/datasets/spambase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataset into a 70-30 split by using the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** While you will use ```train_test_split``` in your homeworks, the code below should help you visualize splitting/masking of a dataframe which will be helpful in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "np.random.seed(42)\n",
    "msk = np.random.rand(len(spam_df)) < 0.7\n",
    "data_train = spam_df[msk]\n",
    "data_test = spam_df[~msk]\n",
    "\n",
    "#Split predictor and response columns\n",
    "x_train, y_train = data_train.drop(['Spam'], axis=1), data_train['Spam']\n",
    "x_test , y_test  = data_test.drop(['Spam'] , axis=1), data_test['Spam']\n",
    "\n",
    "print(\"Shape of Training Set :\",data_train.shape)\n",
    "print(\"Shape of Testing Set :\" ,data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df.iloc[np.arange(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the number of spam cases is roughly evenly represented in both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Percentage of Spam in Train and Test Set\n",
    "percentage_spam_training = 100*y_train.sum()/len(y_train)\n",
    "percentage_spam_testing  = 100*y_test.sum()/len(y_test)\n",
    "                                                  \n",
    "print(\"Percentage of Spam in Training Set \\t : {:0.2f}%.\".format(percentage_spam_training))\n",
    "print(\"Percentage of Spam in Testing Set \\t : {:0.2f}%.\".format(percentage_spam_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "# Part 2 : Fitting an Optimal Single Decision Tree (by Depth) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit here a single tree to our spam dataset and perform 5-fold cross validation on the training set. For EACH depth of the tree, we fit a tree and then compute the 5-fold CV scores. These scores are then averaged and compared across different depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find optimal depth of trees\n",
    "mean_CV_acc = {}\n",
    "all_CV_acc = {}\n",
    "tree_depth_start, tree_depth_end, steps = 3, 31, 4\n",
    "for i in range(tree_depth_start, tree_depth_end, steps):\n",
    "    model = DecisionTreeClassifier(max_depth=i)\n",
    "    score = cross_val_score(estimator=model, X=x_train, y=y_train, cv=5, n_jobs=-1)\n",
    "    all_CV_acc[i] = score\n",
    "    mean_CV_acc[i] = score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_CV_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dictionary manipulations for our x,y construction for the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(mean_CV_acc.keys())\n",
    "y = list(mean_CV_acc.values())\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(mean_CV_acc.items())\n",
    "x, y = zip(*lists)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.ylabel(\"Cross Validation Accuracy\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - Simple Decision Tree')\n",
    "plt.plot(x, y, 'b-', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimal depth is found to be a depth of 7. Although, does it makes sense to choose 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if we wanted to get the Confidence Bands of these results, how would we? It's as simple as a combination of getting variance using ```scores.std()``` and ```plt.fill_between()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = np.array([ np.std(score) for score in all_CV_acc.values() ])\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(x, y + stds, y - stds, alpha=0.2)\n",
    "\n",
    "#Plot\n",
    "plt.ylabel(\"Cross Validation Accuracy\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - Simple Decision Tree')\n",
    "plt.plot(x, y, 'b-', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to display it as a boxplot we first construct a dataframe with all the scores and second we use ```sns.boxplot(...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a numpy array with all the CV acc scores\n",
    "scores_numpy = np.array(list(all_CV_acc.values()))\n",
    "# Making a datafr\n",
    "trees = pd.DataFrame({'Max Depth':x+x+x+x+x, 'CV Accuracy Score':list(scores_numpy[:,0])+\n",
    "                     list(scores_numpy[:,1])+\n",
    "                     list(scores_numpy[:,2])+\n",
    "                     list(scores_numpy[:,3])+\n",
    "                     list(scores_numpy[:,4])})\n",
    "trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the boxplot \n",
    "plt.title('Variation of Accuracy with Depth - Simple Decision Tree')\n",
    "sns.boxplot(x=\"Max Depth\", y=\"CV Accuracy Score\", data=trees);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the boxplot without outliers (showfliers = False)\n",
    "plt.title('Variation of Accuracy with Depth - Simple Decision Tree')\n",
    "sns.boxplot(x=\"Max Depth\", y=\"CV Accuracy Score\", data=trees, showfliers=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's extract the best_depth value from this dictionary:** \n",
    "\n",
    "We create the new variable ```best_depth```. Can you see why we coded the best depth parameter as we did below? (Hint: Think about reproducibility.)\n",
    "\n",
    "How to sort using your own function with key parameter?\n",
    "If you want your own implementation for sorting, sorted() also accepts a key function as an optional parameter.\n",
    "\n",
    "Based on the results of the key function, you can sort the given iterable.\n",
    "\n",
    "```sorted(iterable, key=len)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this do? Is this the result we want?\n",
    "sorted(mean_CV_acc, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this do?\n",
    "sorted(mean_CV_acc, key=mean_CV_acc.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make best depth a variable\n",
    "best_depth = sorted(mean_CV_acc, key=mean_CV_acc.get, reverse=True)[0]\n",
    "print(\"The best depth was found to be:\", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalaute the performance at the best depth\n",
    "model_tree = DecisionTreeClassifier(max_depth=best_depth)\n",
    "model_tree.fit(x_train, y_train)\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_trees_training = accuracy_score(y_train, model_tree.predict(x_train))\n",
    "acc_trees_testing  = accuracy_score(y_test,  model_tree.predict(x_test))\n",
    "\n",
    "print(\"Simple Decision Trees: Accuracy, Training Set \\t : {:.2%}\".format(acc_trees_training))\n",
    "print(\"Simple Decision Trees: Accuracy, Testing Set \\t : {:.2%}\".format(acc_trees_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Performance by Class (Lookup Confusion Matrix)\n",
    "pd.crosstab(y_test, model_tree.predict(x_test), margins=True, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to visualize a Decision Tree with ```pydot```\n",
    "\n",
    "*Question:* Do you think this tree is interpretable? What do you think about a the maximal depth of the tree?\n",
    "\n",
    "- Let's first store the decision tree in a text format: ```decision_tree.dot```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "file_name = \"results/decision_tree.dot\"\n",
    "tree.export_graphviz(model_tree, out_file = file_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look at the resulting text ```decision_tree.dot```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head results/decision_tree.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's convert our (hard to read) written decision tree (```decision_tree.dot```) into an intuitive image file format: ```image_tree.png```\n",
    "- <span style=\"color:red\">**NOTE:**</span> You might need to install the ```pydot``` package by typing the following command in your terminal: ```pip install pydot``` or you can install from within the jupyter notebook by running the following cell: ```! pip install pydot```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydot\n",
    "! conda install graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "(graph,) = pydot.graph_from_dot_file(file_name)\n",
    "graph.write_png('results/image_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's display the ```image_tree.png``` in markdown:\n",
    "\n",
    "Markdown: ```![title](results/image_tree.png)```. The result: \n",
    "\n",
    "![title](results/image_tree.png)\n",
    "\n",
    "*Repost Question:* Do you think this tree is interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------\n",
    "\n",
    "\n",
    "# Part 3: Bagging and Voting\n",
    "\n",
    "*QUESTION:* Where does the word *\"Bagging\"* come from?\n",
    "\n",
    "#### Some Theory: What is bagging?\n",
    "  1. Bootstrapping: resample with replacements to get different datasets and built different models.\n",
    "  2. Do something smart to combine the different models.\n",
    "  \n",
    "One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. \n",
    "\n",
    " 1. **Bootstrap:** we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. \n",
    " 2. **AGgregatiING** for a given input, we output the averaged outputs of all the models for that input. \n",
    " \n",
    "This method is called **Bagging: B** ootstrap + **AGG**regat**ING**. \n",
    "\n",
    "-----------\n",
    "\n",
    "Let's bootstrap our training dataset to create multiple datasets and fit Decision Tree models to each.\n",
    "\n",
    "(Resampling: we showed live that different samples give different results for things like sums, varying more when the things we sum over have high variance themselves.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat on all data\n",
    "data_train.mean(axis=0).to_frame('mean').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.sample(frac=1., replace=True).mean(axis=0).to_frame('mean').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually fit the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 100 # we tried a variety of numbers here\n",
    "choosen_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating model\n",
    "np.random.seed(0)\n",
    "model = DecisionTreeClassifier(max_depth=choosen_depth)\n",
    "\n",
    "#Initializing variables\n",
    "predictions_train = np.zeros((data_train.shape[0], n_trees))\n",
    "predictions_test = np.zeros((data_test.shape[0], n_trees))\n",
    "\n",
    "#Conduct bootstraping iterations\n",
    "for i in range(n_trees):\n",
    "    temp = data_train.sample(frac=1, replace=True)\n",
    "    response_variable = temp['Spam']\n",
    "    temp = temp.drop(['Spam'], axis=1)\n",
    "    \n",
    "    model.fit(temp, response_variable)  \n",
    "    predictions_train[:,i] = model.predict(x_train)   \n",
    "    predictions_test[:,i] = model.predict(x_test)\n",
    "    \n",
    "#Make Predictions Dataframe\n",
    "columns = [\"Bootstrap-Model_\"+str(i+1) for i in range(n_trees)]\n",
    "predictions_train = pd.DataFrame(predictions_train, columns=columns)\n",
    "predictions_test = pd.DataFrame(predictions_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['Spam'].values\n",
    "y_test = data_test['Spam'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example Bolean for locating the Non Spam\n",
    "y == 0\n",
    "## Example Bolean for locating the Spam\n",
    "y == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_avg = 100\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 7))\n",
    "for (ax, label, predictions, y) in [\n",
    "    (axs[0], 'Training Set', predictions_train, y_train), \n",
    "    (axs[1], 'Test Set' , predictions_test , y_test ) ]:\n",
    "    \n",
    "    # Take the average\n",
    "    mean_predictions = predictions.iloc[:,:num_to_avg].mean(axis=1)\n",
    "    \n",
    "    # Plot the Spam\n",
    "    mean_predictions[y == 1].hist(density=True, histtype='step', \n",
    "                                  range=[0,1], label='Spam', lw=2, ax=ax)\n",
    "    \n",
    "    # Plot the non Spam\n",
    "    mean_predictions[y == 0].hist(density=True, histtype='step', \n",
    "                                  range=[0,1], label='Not-Spam', lw=2, ax=ax)\n",
    "    ax.legend(loc='upper center');\n",
    "    ax.set_xlabel(\"Mean of ensemble predictions (0.5=50 True - 50 False)\")\n",
    "    ax.set_title(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now get final predictions: majority voting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ensemble the prediction of each bagged decision tree model\n",
    "def get_prediction(df, count=-1):\n",
    "    count = df.shape[1] if count==-1 else count\n",
    "    temp = df.iloc[:,0:count]\n",
    "    return np.mean(temp, axis=1)>0.5\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_bagging_training = 100*accuracy_score(y_train, get_prediction(predictions_train, count=-1))\n",
    "acc_bagging_testing  = 100*accuracy_score(y_test, get_prediction(predictions_test, count=-1))\n",
    "\n",
    "print(\"Bagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count in the above code can be use to define the number of models the voting in the dataframe should be based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Performance by Class (Lookup Confusion Matrix)\n",
    "pd.crosstab(np.array(y_test), model.predict(x_test), margins=True, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Food for Thought :** Are these bagging models independent of each other, can they be trained in a parallel fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Random Forest vs Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Random Forest? \n",
    "\n",
    "- **Many trees** make a **forest**.\n",
    "- **Many random trees** make a **random forest**.\n",
    "\n",
    "\n",
    "Random Forest is a modified form of bagging that creates ensembles of independent decision trees. \n",
    "To *de-correlate the trees*, we: \n",
    "1. train each tree on a separate bootstrap **random sample** of the full training set (same as in bagging) \n",
    "2. for each tree, at each split, we **randomly select a set of ùêΩ‚Ä≤ predictors from the full set of predictors.** (not done in bagging)\n",
    "3. From amongst the ùêΩ‚Ä≤  predictors, we select the optimal predictor and the optimal corresponding threshold for the split. \n",
    "\n",
    "*Question:* Why would this second step help (only considering random sub-group of features)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit an ensemble method, the Random Forest technique, which is different from the decision tree. Refer to the lectures slides for a full treatment on how they are different. Let's use ```n_estimators = predictor_count/2``` and ```max_depth = best_depth```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Random Forest Model\n",
    "\n",
    "#Training\n",
    "model = RandomForestClassifier(n_estimators=int(x_train.shape[1]/2), max_depth=best_depth)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "acc_random_forest_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_random_forest_testing = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Random Forest: Accuracy, Training Set : {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: Accuracy, Testing Set :  {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare the performance of our 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Training Set \\t: {:.2%}\".format(acc_trees_training))\n",
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "\n",
    "print(\"\\nBagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "\n",
    "print(\"\\nRandom Forest: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we see above, the performance of both Bagging and Random Forest was similar, so what is the difference? Do both overfit the data just as much?\n",
    "\n",
    "Hints :\n",
    "\n",
    "- What is the only extra parameter we declared when defining a Random Forest Model vs Bagging? Does it have an impact on overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Random Forest Model\n",
    "\n",
    "new_depth = best_depth + 20 \n",
    "\n",
    "#Training\n",
    "model = RandomForestClassifier(n_estimators=int(x_train.shape[1]/2), max_depth=new_depth)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Perfromance Evaluation\n",
    "acc_random_forest_deeper_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_random_forest_deeper_testing = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Random Forest: Accuracy, Training Set (Deeper): {:0.2f}%\".format(acc_random_forest_deeper_training))\n",
    "print(\"Random Forest: Accuracy, Testing Set (Deeper):  {:0.2f}%\".format(acc_random_forest_deeper_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracies:\")\n",
    "print(\"Decision Trees:\\tAccuracy, Training Set \\t: {:.2%}\".format(acc_trees_training))\n",
    "print(\"Bagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Random Forest: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"RF Deeper: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_deeper_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Accuracies:\")\n",
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"RF Deeper:  \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_deeper_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "Random Forest gives the above values as ```feature_importance``` where it normalizes the impact of a predictor to the number of times it is useful and thus gives overvall significance for free. Explore the attributes of the Random Forest model object for the best nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Features\n",
    "feature_importance = model.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, x_train.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Initial Questions:\n",
    "\n",
    "- What are decision tree models?\n",
    "- How do we construct them?\n",
    "- How do we vizualize them?\n",
    "- What is the idea of bagging?\n",
    "- Why does bagging help with overfitting?\n",
    "- Why does bagging help to built more expressive trees?\n",
    "\n",
    "#### End of Standard Section. What about next week? \n",
    "\n",
    "**Gradient Boosting etc.. building upon decision trees. Why should we care?** \n",
    "\n",
    "<img src=\"data/kaggle.png\" alt=\"tree_adj\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "----------\n",
    "<img src=\"data/trees_adj.png\" alt=\"tree_adj\" width=\"100%\"/>\n",
    "\n",
    "[Source Medion: \"XGBoost Algorithm: Long May She Reign!\"](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
